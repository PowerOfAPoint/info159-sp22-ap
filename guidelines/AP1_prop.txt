Zainab Hossainzadeh
Salmonn Talebi
Steven Yuan
For our annotation project, we will investigate how “professional-sounding” a piece of text is. While the standards for professionality depend heavily on context, for this project we define a highly professional document as one that has an authoritative tone and uses precise, academic language; on the other end, a highly unprofessional document is one that has a casual tone and uses loose, everyday language. For every document in our data set, we will assign a value that represents how professional it is according to our guidelines. Our end goal for this project is to have a model that can identify what components of a piece of text make it “professional” or “unprofessional” and also determine how professional any given document is.
We believe this is important to investigate because in many contexts there is an implicit correlation between professionalism and trustworthiness. For example, when deciding whether to buy a product, one will probably not entirely base one’s decision on a review that is overly emotional or lacking in specific details. Or, in an online discussion thread, perhaps one will be more likely to believe the author of a comment that reflects a high level of maturity and thoughtfulness toward the subject at hand. We rely heavily these days on opinions we read on the internet, and the level of professionalism plays a role in how much we trust what we are reading.
Our data source will likely consist of a mixture of Reddit comments, specifically from r/news, and Amazon product reviews, specifically reviews for products categorized under Books. This is so that we can ensure we get a wide range of contexts for our model to train on. Both sources we will be using are neither private nor under copyright and can be freely shared with others.